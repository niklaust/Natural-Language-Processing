{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "lv8fhPoR3sCt"
      ],
      "authorship_tag": "ABX9TyMzt8FgQLQmzLWpQc7JY7g/",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/niklaust/Natural_Language_Processing/blob/main/NLTK_for_NLP_notebook_of_niklaust.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h1><center><b>NLTK with Python 3 for Natural Language Processing</b></center></h1>\n"
      ],
      "metadata": {
        "id": "L3pKWgARrxuW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "youtube: https://www.youtube.com/playlist?list=PLQVvvaa0QuDf2JswnfiGkliBInZnIC4HL\n",
        "\n",
        "teacher: sentdex"
      ],
      "metadata": {
        "id": "oDyCBQhLsCH-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "github:niklaust"
      ],
      "metadata": {
        "id": "FaXC-R8Jr6It"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "start 20230220"
      ],
      "metadata": {
        "id": "xJHeR7PStSkm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **1. Tokenizing words and Sentences**"
      ],
      "metadata": {
        "id": "CmVHBGA1rVAW"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VYG75DG6rJZV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0c8ffaf1-f6c0-4f03-c589-9267d2d81a5f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "NLTK Downloader\n",
            "---------------------------------------------------------------------------\n",
            "    d) Download   l) List    u) Update   c) Config   h) Help   q) Quit\n",
            "---------------------------------------------------------------------------\n",
            "Downloader> q\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ],
      "source": [
        "import nltk\n",
        "\n",
        "nltk.download()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "* **tokenizing** - a form of grouping things: word tokenizers(separates by words), sentence tokenizers(separate by sentence).\n",
        "\n",
        "* **corporas** - a body of text. ex: medical journals, presidential speeches, English language\n",
        "* **lexicon** - like a dictionary. words and their means (STOCK MARKET 'bull' = a person who buys shares hoping to sell them at a higher price later., zoology 'bull' = an uncastrated male bovine animal.)"
      ],
      "metadata": {
        "id": "y0CsYMjFwENF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('punkt')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eV7e-kCWzKaI",
        "outputId": "e1900454-5e58-4a4a-d7a9-2d8c4cfa05fc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import sent_tokenize, word_tokenize\n",
        "\n",
        "example_text = \"\"\"Hello Mr.Smith, how are you doing today? \n",
        "The weather is great and Python is awesome. \n",
        "The sky is pinkish-blue. \n",
        "you should not eat cardboard.\"\"\""
      ],
      "metadata": {
        "id": "RLyV8fQ3vzA2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(sent_tokenize(example_text))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZGvhCmmnzo9N",
        "outputId": "a2f0ec04-6c3e-462c-d6f0-ac14e63361dc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Hello Mr.Smith, how are you doing today?', 'The weather is great and Python is awesome.', 'The sky is pinkish-blue.', 'you should not eat cardboard.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(word_tokenize(example_text))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y1AZnUczzqa2",
        "outputId": "b9308ea6-3cae-46af-cccb-e83ad3c18e0d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Hello', 'Mr.Smith', ',', 'how', 'are', 'you', 'doing', 'today', '?', 'The', 'weather', 'is', 'great', 'and', 'Python', 'is', 'awesome', '.', 'The', 'sky', 'is', 'pinkish-blue', '.', 'you', 'should', 'not', 'eat', 'cardboard', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for i in word_tokenize(example_text):\n",
        "  print(i)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MLepv_gmz46_",
        "outputId": "5889410a-212e-4313-cd5c-224dcdaa0ecd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hello\n",
            "Mr.Smith\n",
            ",\n",
            "how\n",
            "are\n",
            "you\n",
            "doing\n",
            "today\n",
            "?\n",
            "The\n",
            "weather\n",
            "is\n",
            "great\n",
            "and\n",
            "Python\n",
            "is\n",
            "awesome\n",
            ".\n",
            "The\n",
            "sky\n",
            "is\n",
            "pinkish-blue\n",
            ".\n",
            "you\n",
            "should\n",
            "not\n",
            "eat\n",
            "cardboard\n",
            ".\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "valyrio = \"\"\"Nyke Daenerys Jelmazmo \n",
        "hen Targario Lentrot,\n",
        "hen Valyrio Uepo anogar iksan.\n",
        "Valyrio muño engos ñuhys issa.\n",
        "Dovaogedys!\n",
        "Aeksia ossenatas,\n",
        "menti ossenatas,\n",
        "qiloni pilos lue vale tolvie ossenatas,\n",
        "yn riñe dore odrikatas.\n",
        "Urnet luo buzdaro tolvio belma pryjatas!\"\"\"\n",
        "\n",
        "print(sent_tokenize(valyrio.replace('\\n', ' ')))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rQxFfpyx042d",
        "outputId": "347af77d-6278-4d90-b925-65e6c01541e4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Nyke Daenerys Jelmazmo  hen Targario Lentrot, hen Valyrio Uepo anogar iksan.', 'Valyrio muño engos ñuhys issa.', 'Dovaogedys!', 'Aeksia ossenatas, menti ossenatas, qiloni pilos lue vale tolvie ossenatas, yn riñe dore odrikatas.', 'Urnet luo buzdaro tolvio belma pryjatas!']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **2.Stop Words**"
      ],
      "metadata": {
        "id": "lv8fhPoR3sCt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Stop words -  the common **words that are often removed from texts** during pre-processing, as they are considered to carry little to no meaning or have a low semantic value.\n",
        "\n",
        "Examples of stop words include words like \"the\", \"a\", \"an\", \"in\", \"and\", \"is\", \"of\", \"to\", \"that\", and \"it\".  These words often occur frequently in natural language text, but typically do not carry much important meaning or context, and their inclusion can add noise to analysis or machine learning models.\n",
        "\n",
        "By removing stop words from text, the remaining words can be more representative of the meaning and context of the text, which can help improve the accuracy and efficiency of natural language processing tasks such as text classification, sentiment analysis, and information retrieval."
      ],
      "metadata": {
        "id": "ICqN2bku30P9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('stopwords')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4F27tTME5q6d",
        "outputId": "82765797-cc82-4e19-9272-cb6ac416a078"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "example_sentence = \"This is an example showing off stop word filtration.\"\n",
        "\n",
        "stop_words = set(stopwords.words(\"english\"))\n",
        "\n",
        "words = word_tokenize(example_sentence)\n",
        "\n",
        "filtered_sentence = []\n",
        "\n",
        "for w in words:\n",
        "  if w not in stop_words:\n",
        "    filtered_sentence.append(w) \n",
        "\n",
        "print(filtered_sentence)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "emJiVkcy3fVn",
        "outputId": "92792c18-8d00-43c6-c2ff-534c87ed3608"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['This', 'example', 'showing', 'stop', 'word', 'filtration', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "filtered_sentence = [w for w in words if w not in stop_words]\n",
        "\n",
        "print(filtered_sentence)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "biu9ALkc51Qt",
        "outputId": "e504b8dc-22ea-47d8-efa7-46db2cc00e32"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['This', 'example', 'showing', 'stop', 'word', 'filtration', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "meningar = \"\"\"Jag lever livet i nuet.\n",
        "Jag tror att det är bra.\n",
        "Varje minut ät \n",
        "viktig för mig.\n",
        "Det är bra att ha \n",
        "tid för barnen \n",
        "och familjen.\n",
        "Den tiden är \n",
        "mycket viktig \n",
        "för mig.\n",
        "\"\"\"\n",
        "\n",
        "stoppa_ord = list(stopwords.words(\"swedish\")) + list('.')\n",
        "ord = word_tokenize(meningar)\n",
        "print(ord)\n",
        "\n",
        "filtrerad_sats = [o for o in ord if o not in stoppa_ord]\n",
        "\n",
        "print(filtrerad_sats)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nrUecNqL7Aet",
        "outputId": "0fccd509-bc4e-46ce-e048-67371b768a8c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Jag', 'lever', 'livet', 'i', 'nuet', '.', 'Jag', 'tror', 'att', 'det', 'är', 'bra', '.', 'Varje', 'minut', 'ät', 'viktig', 'för', 'mig', '.', 'Det', 'är', 'bra', 'att', 'ha', 'tid', 'för', 'barnen', 'och', 'familjen', '.', 'Den', 'tiden', 'är', 'mycket', 'viktig', 'för', 'mig', '.']\n",
            "['Jag', 'lever', 'livet', 'nuet', 'Jag', 'tror', 'bra', 'Varje', 'minut', 'ät', 'viktig', 'Det', 'bra', 'tid', 'barnen', 'familjen', 'Den', 'tiden', 'viktig']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **3.Stemming**"
      ],
      "metadata": {
        "id": "pEqRleaZt_XN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Stemming** is the process of reducing words to their **base or root form**, in order to normalize them and group them together for analysis. Stemming is commonly used as a preprocessing step in text analysis, to **reduce the number of unique words** that need to be analyzed and improve the accuracy of the analysis.\n",
        "\n",
        "For example, the word \"running\" might be stemmed to \"run\", and the word \"cats\" might be stemmed to \"cat\".\n",
        "\n",
        "\n",
        "* I was taking a ride in the car.  (ride - noun) \n",
        "* I was riding in the car.         (ride - verb)"
      ],
      "metadata": {
        "id": "56msSg2fuuvd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qv0CWvyewYa8",
        "outputId": "dfe06a94-95ec-46e8-a026-59699c910e41"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import PorterStemmer\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "ps = PorterStemmer()\n",
        "\n",
        "example_words = [\"python\", \"pythoner\", \"pythoning\", \"pythoned\", \"pythonly\"]\n",
        "\n",
        "for w in example_words:\n",
        "  print(ps.stem(w))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sPr4yLy4vwgE",
        "outputId": "42e969c2-3e85-490f-967b-490dc18bb15e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "python\n",
            "python\n",
            "python\n",
            "python\n",
            "pythonli\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "new_text = \"\"\"\n",
        "It is very important to be pythonly\n",
        "while you are pythoning with python. \n",
        "All pythoners have pythoned poorly \n",
        "at least once.\n",
        "\"\"\"\n",
        "words = word_tokenize(new_text)\n",
        "\n",
        "for w in words:\n",
        "  print(ps.stem(w))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "srtErj3X0T9F",
        "outputId": "3b563339-4f0d-4e72-b7ea-698206f693c6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "it\n",
            "is\n",
            "veri\n",
            "import\n",
            "to\n",
            "be\n",
            "pythonli\n",
            "while\n",
            "you\n",
            "are\n",
            "python\n",
            "with\n",
            "python\n",
            ".\n",
            "all\n",
            "python\n",
            "have\n",
            "python\n",
            "poorli\n",
            "at\n",
            "least\n",
            "onc\n",
            ".\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem.snowball import SnowballStemmer\n",
        "\n",
        "stemmer = SnowballStemmer('spanish')\n",
        "\n",
        "frase = \"La cantantita le cantó una canción al cantantillo.\"\n",
        "palabras = word_tokenize(frase)\n",
        "\n",
        "# stemming technique \n",
        "palabras_lematizadas = [stemmer.stem(palabra) for palabra in palabras]\n",
        "\n",
        "print(palabras_lematizadas)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IRQQFShl2t_W",
        "outputId": "56d890cd-8d65-4419-f6ef-be04da379621"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['la', 'cantantit', 'le', 'cant', 'una', 'cancion', 'al', 'cantantill', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "import spacy.cli\n",
        "\n",
        "spacy.cli.download(\"es_core_news_sm\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tM2IFndS79rj",
        "outputId": "285a6740-e9c6-481a-940c-3d790653dd55"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('es_core_news_sm')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nlp = spacy.load(\"es_core_news_sm\")\n",
        "\n",
        "doc = nlp(\"La cantantita le cantó una canción al cantantillo\")\n",
        "\n",
        "# lemmatization technique\n",
        "lemmatized_words = [token.lemma_ for token in doc]\n",
        "\n",
        "print(lemmatized_words)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eEflBmWB7Ue0",
        "outputId": "f0412b85-f32d-4c63-eb0b-6c7107523acf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['el', 'cantantita', 'él', 'cantar', 'uno', 'canción', 'al', 'cantantillo']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **4. Part of Speech Tagging**"
      ],
      "metadata": {
        "id": "WNllMMA26bhj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<table>\n",
        "  <tr>\n",
        "    <th colspan=\"2\"><h4>Part-of-Speech(POS) tag</h4></th>\n",
        "  </tr>\n",
        "    <th></th>\n",
        "    <th></th>\n",
        "  </tr>\n",
        "  <tr>\n",
        "    <td>CC:</td>\n",
        "    <td>coordinating conjunction</td>\n",
        "  </tr>\n",
        "  <tr>\n",
        "    <td>CD:</td>\n",
        "    <td>cardinal digit</td>\n",
        "  </tr>\n",
        "  <tr>\n",
        "    <td>DT:</td>\n",
        "    <td>determiner</td>\n",
        "  </tr>\n",
        "  <tr>\n",
        "    <td>EX:</td>\n",
        "    <td>existential there (like: \"there is\" ... think of it like \"there exists\")</td>\n",
        "  </tr>\n",
        "  <tr>\n",
        "    <td>FW:</td>\n",
        "    <td>foreign word</td>\n",
        "  </tr>\n",
        "  <tr>\n",
        "    <td>IN:</td>\n",
        "    <td>preposition/subordinating conjunction</td>\n",
        "  </tr>\n",
        "  <tr>\n",
        "    <td>JJ:</td>\n",
        "    <td>adjective 'big'</td>\n",
        "  </tr>\n",
        "  <tr>\n",
        "    <td>JJR:</td>\n",
        "    <td>adjective, comparative 'bigger'</td>\n",
        "  </tr>\n",
        "  <tr>\n",
        "    <td>JJS:</td>\n",
        "    <td>adjective, superlative 'biggest'</td>\n",
        "  </tr>\n",
        "  <tr>\n",
        "    <td>LS:</td>\n",
        "    <td>list marker 1)</td>\n",
        "  </tr>\n",
        "  <tr>\n",
        "    <td>MD:</td>\n",
        "    <td>modal could, will</td>\n",
        "  </tr>\n",
        "  <tr>\n",
        "    <td>NN:</td>\n",
        "    <td>noun, singular 'desk'</td>\n",
        "  </tr>\n",
        "  <tr>\n",
        "    <td>NNS:</td>\n",
        "    <td>noun plural 'desks'</td>\n",
        "  </tr>\n",
        "  <tr>\n",
        "    <td>NNP:</td>\n",
        "    <td>proper noun, singular 'Harrison'</td>\n",
        "  </tr>\n",
        "  <tr>\n",
        "    <td>NNPS:</td>\n",
        "    <td>proper noun, plural 'Americans'</td>\n",
        "  </tr>\n",
        "  <tr>\n",
        "    <td>PDT:</td>\n",
        "    <td>predeterminer 'all the kids'</td>\n",
        "  </tr>\n",
        "  <tr>\n",
        "    <td>POS:</td>\n",
        "    <td>possessive ending parent's</td>\n",
        "  </tr>\n",
        "  <tr>\n",
        "    <td>PRP:</td>\n",
        "    <td>personal pronoun I, he, she</td>\n",
        "  </tr>\n",
        "  <tr>\n",
        "    <td>PRP$:</td>\n",
        "    <td>possessive pronoun my, his, hers</td>\n",
        "  </tr>\n",
        "  <tr>\n",
        "    <td>RB:</td>\n",
        "    <td>adverb very, silently,</td>\n",
        "  </tr>\n",
        "  <tr>\n",
        "    <td>RBR:</td>\n",
        "    <td>adverb, comparative better</td>\n",
        "  </tr>\n",
        "  <tr>\n",
        "    <td>RBS:</td>\n",
        "    <td>adverb, superlative best</td>\n",
        "  </tr>\n",
        "  <tr>\n",
        "    <td>RP:</td>\n",
        "    <td>particle give up</td>\n",
        "  </tr>\n",
        "  <tr>\n",
        "    <td>TO:</td>\n",
        "    <td>to go 'to' the store.</td>\n",
        "  </tr>\n",
        "  <tr>\n",
        "    <td>UH:</td>\n",
        "    <td>interjection uhhuhhuhh</td>\n",
        "  </tr>\n",
        "  <tr>\n",
        "    <td>VB:</td>\n",
        "    <td>verb, base form take</td>\n",
        "  </tr>\n",
        "  <tr>\n",
        "    <td>VBD:</td>\n",
        "    <td>verb, past tense took</td>\n",
        "  </tr>\n",
        "  <tr>\n",
        "    <td>VBG:</td>\n",
        "    <td>verb, gerund/present participle taking</td>\n",
        "  </tr>\n",
        "  <tr>\n",
        "    <td>VBN:</td>\n",
        "    <td>verb, past participle taken</td>\n",
        "  </tr>\n",
        "  <tr>\n",
        "    <td>VBP:</td>\n",
        "    <td>verb, sing. present, non-3d take</td>\n",
        "  </tr>\n",
        "  <tr>\n",
        "    <td>VBZ:</td>\n",
        "    <td>verb, 3rd person sing. present takes</td>\n",
        "  </tr>\n",
        "  <tr>\n",
        "    <td>WDT:</td>\n",
        "    <td>wh-determiner which</td>\n",
        "  </tr>\n",
        "  <tr>\n",
        "    <td>WP:</td>\n",
        "    <td>wh-pronoun who, what</td>\n",
        "  </tr>\n",
        "  <tr>\n",
        "    <td>WP$:</td>\n",
        "    <td>possessive wh-pronoun whose</td>\n",
        "  </tr>\n",
        "  <tr>\n",
        "    <td>WRB:</td>\n",
        "    <td>wh-abverb where, when</td>\n",
        "  </tr>\n",
        "</table>"
      ],
      "metadata": {
        "id": "wvDpu1AaBIik"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('state_union')\n",
        "nltk.download('averaged_perceptron_tagger')   # pos_tag"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HFew_XFi-4ND",
        "outputId": "92e57845-065f-4e60-dc7c-d957c3e4258a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package state_union to /root/nltk_data...\n",
            "[nltk_data]   Package state_union is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "`PunktSentenceTokenizer` is a sentence tokenizer in NLTK.\n",
        "It is based on unsupervised machine learning algorithm Punkt. It can be trained on a large corpus of text to determine sentence boundaries. Once trained, it can be used to tokenize new text into sentences for further analysis."
      ],
      "metadata": {
        "id": "Nk08jHX0OCt7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import state_union\n",
        "from nltk.tokenize import PunktSentenceTokenizer\n",
        "\n",
        "train_text = state_union.raw(\"2005-GWBush.txt\")\n",
        "sample_text = state_union.raw(\"2006-GWBush.txt\")\n",
        "\n",
        "# create a custom sentence tokenizer based on\n",
        "# the training text from the 2005 speech\n",
        "custom_sent_tokenizer = PunktSentenceTokenizer(train_text)\n",
        "\n",
        "# tokenizes the sample text using the custom sentence tokenizer\n",
        "tokenized = custom_sent_tokenizer.tokenize(sample_text)\n",
        "\n",
        "def process_countent():\n",
        "  try:\n",
        "      for i in tokenized:\n",
        "        words = nltk.word_tokenize(i)\n",
        "        tagged = nltk.pos_tag(words)   # pos tagging on text data.\n",
        "        print(tagged)\n",
        "\n",
        "  except Exception as e:\n",
        "    print(str(e))"
      ],
      "metadata": {
        "id": "SNqeyxlA9V4M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenized[:5]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7UECL5vtFUjr",
        "outputId": "388edf92-4615-4484-c4e8-697c4f9e27a1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[\"PRESIDENT GEORGE W. BUSH'S ADDRESS BEFORE A JOINT SESSION OF THE CONGRESS ON THE STATE OF THE UNION\\n \\nJanuary 31, 2006\\n\\nTHE PRESIDENT: Thank you all.\",\n",
              " 'Mr. Speaker, Vice President Cheney, members of Congress, members of the Supreme Court and diplomatic corps, distinguished guests, and fellow citizens: Today our nation lost a beloved, graceful, courageous woman who called America to its founding ideals and carried on a noble dream.',\n",
              " 'Tonight we are comforted by the hope of a glad reunion with the husband who was taken so long ago, and we are grateful for the good life of Coretta Scott King.',\n",
              " '(Applause.)',\n",
              " 'President George W. Bush reacts to applause during his State of the Union Address at the Capitol, Tuesday, Jan.']"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "process_countent()"
      ],
      "metadata": {
        "id": "ZTlxUFlKFADE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **5. Chunking**"
      ],
      "metadata": {
        "id": "CMBwD2hoC7JT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Chunking is the process of **extracting** meaningful phrases (or \"chunks\") **from a text by identifying and labeling sequences of words** that have a specific grammatical structure."
      ],
      "metadata": {
        "id": "y_74LmRVQDLE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk"
      ],
      "metadata": {
        "id": "Z0XUVaGeNSz1"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('punkt')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yBZhVX8bFfF4",
        "outputId": "9bff0e51-bc55-46ca-8a17-c7853336bbd4"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('state_union')\n",
        "nltk.download('averaged_perceptron_tagger')   # pos_tag"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jOuY204NNYtm",
        "outputId": "db7cc5ea-02ee-4ea7-a0dc-28afe3878ff1"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package state_union to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/state_union.zip.\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.tokenize import word_tokenize"
      ],
      "metadata": {
        "id": "veXgba47IbVu"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.corpus import state_union\n",
        "from nltk.tokenize import PunktSentenceTokenizer\n",
        "\n",
        "train_text = state_union.raw(\"2005-GWBush.txt\")\n",
        "sample_text = state_union.raw(\"2006-GWBush.txt\")\n",
        "\n",
        "# create a custom sentence tokenizer based on\n",
        "# the training text from the 2005 speech\n",
        "custom_sent_tokenizer = PunktSentenceTokenizer(train_text)\n",
        "\n",
        "# tokenizes the sample text using the custom sentence tokenizer\n",
        "tokenized = custom_sent_tokenizer.tokenize(sample_text)"
      ],
      "metadata": {
        "id": "I2dOdas_Neen"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def process_countent():\n",
        "  for i in tokenized:\n",
        "    words = nltk.word_tokenize(i)\n",
        "    tagged = nltk.pos_tag(words)\n",
        "\n",
        "    chunkGram = r\"\"\"Chunk: {<RB.?>*<VB.?>*<NNP>+<NN>?}\"\"\"\n",
        "\n",
        "    chunkParser = nltk.RegexpParser(chunkGram)\n",
        "    chunked = chunkParser.parse(tagged)\n",
        "\n",
        "    print(chunked)"
      ],
      "metadata": {
        "id": "Q5tdgyBcD94I"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "process_countent()"
      ],
      "metadata": {
        "id": "vrkAvRJtFQ9y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **6. Chinking**"
      ],
      "metadata": {
        "id": "esWUcVPDOAMt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Chinking is the process of **removing** a sequence of tokens (or \"chunks\") **from a larger text that match a specific pattern.** The pattern is defined using a regular expression, just like in chunking."
      ],
      "metadata": {
        "id": "a-g-OtXbQfnM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk"
      ],
      "metadata": {
        "id": "qlXrr6zpOjcW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('punkt')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LMo00UcfOlF9",
        "outputId": "f3a3ae75-492e-4a8f-d8e2-870de5ba70d5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('state_union')\n",
        "nltk.download('averaged_perceptron_tagger')   # pos_tag"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "chMhFe8AOmoF",
        "outputId": "fae2adb6-3538-41b2-b0e4-b5df155dfa43"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package state_union to /root/nltk_data...\n",
            "[nltk_data]   Package state_union is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.tokenize import word_tokenize"
      ],
      "metadata": {
        "id": "RLJgfQ3JOoQ_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.corpus import state_union\n",
        "from nltk.tokenize import PunktSentenceTokenizer\n",
        "\n",
        "train_text = state_union.raw(\"2005-GWBush.txt\")\n",
        "sample_text = state_union.raw(\"2006-GWBush.txt\")\n",
        "\n",
        "# create a custom sentence tokenizer based on\n",
        "# the training text from the 2005 speech\n",
        "custom_sent_tokenizer = PunktSentenceTokenizer(train_text)\n",
        "\n",
        "# tokenizes the sample text using the custom sentence tokenizer\n",
        "tokenized = custom_sent_tokenizer.tokenize(sample_text)"
      ],
      "metadata": {
        "id": "I-UwTCUUOp6U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def process_countent():\n",
        "  for i in tokenized:\n",
        "    words = nltk.word_tokenize(i)\n",
        "    tagged = nltk.pos_tag(words)\n",
        "\n",
        "    chunkGram = r\"\"\"Chunk: {<.*>+}\n",
        "                            }<VB.?|IN|DT|TO>+{\"\"\"\n",
        "\n",
        "    chunkParser = nltk.RegexpParser(chunkGram)\n",
        "    chunked = chunkParser.parse(tagged)\n",
        "\n",
        "    print(chunked)"
      ],
      "metadata": {
        "id": "JhvSkAHAOrcV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "process_countent()"
      ],
      "metadata": {
        "id": "t4mOAdK2O9mo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **7. Named Entity Recognition**"
      ],
      "metadata": {
        "id": "AJq07sDDBZUs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<table>\n",
        "  <tr>\n",
        "    <th colspan=\"9\"><h4>Commonly used types of named entity</h4></th>\n",
        "  </tr>\n",
        "  <tr>\n",
        "    <th>NE type</th>\n",
        "    <th>Examples</th>\n",
        "  </tr>\n",
        "  <tr>\n",
        "    <td>ORGANIZATION</td>\n",
        "    <td>Georgia-Pacific Corp., WHO</td>\n",
        "  </tr>\n",
        "  <tr>\n",
        "    <td>PERSON</td>\n",
        "    <td>Eddy Bonte, Persident Obama</td>\n",
        "  </tr>\n",
        "  <tr>\n",
        "    <td>LOCATION</td>\n",
        "    <td>Murray River, Mount Everest</td>\n",
        "  </tr>\n",
        "  <tr>\n",
        "    <td>DATE</td>\n",
        "    <td>June, 2008-06-29</td>\n",
        "  </tr>\n",
        "  <tr>\n",
        "    <td>TIME</td>\n",
        "    <td>two fifty a m, 1:30 p.m.</td>\n",
        "  </tr>\n",
        "  <tr>\n",
        "    <td>MONEY</td>\n",
        "    <td>175 million Canadian Dollars, GBP 10.40</td>\n",
        "  </tr>\n",
        "  <tr>\n",
        "    <td>PERCENT</td>\n",
        "    <td>twenty pct, 18.75%</td>\n",
        "  </tr>\n",
        "  <tr>\n",
        "    <td>FACILITY</td>\n",
        "    <td>Washington Monument, Stonehenge</td>\n",
        "  </tr>\n",
        "  <tr>\n",
        "    <td>GPE</td>\n",
        "    <td>South East Asia, Midlothian</td>\n",
        "  </tr>\n",
        "</table>"
      ],
      "metadata": {
        "id": "nEk6uWJkEI50"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk"
      ],
      "metadata": {
        "id": "4BdEmHWmBoz8"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('punkt')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1oQJiId6BqZD",
        "outputId": "f06dc999-2ca0-44d4-b175-d50a3b341c85"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('state_union')\n",
        "nltk.download('averaged_perceptron_tagger')   # pos_tag"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eEDWlRcvBr11",
        "outputId": "dd80a44a-bd11-4d85-b0c5-b92903bc94f7"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package state_union to /root/nltk_data...\n",
            "[nltk_data]   Package state_union is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('words')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I_IoqkHRCXhk",
        "outputId": "77aae1d4-5834-45b9-edc5-658b64716dc9"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package words to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/words.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('maxent_ne_chunker')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eJGWOdZGCRY0",
        "outputId": "94073133-1347-4f6d-d744-f4c9ca170fe6"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package maxent_ne_chunker to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping chunkers/maxent_ne_chunker.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.tokenize import word_tokenize"
      ],
      "metadata": {
        "id": "IeN-3i7MBtc9"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.corpus import state_union\n",
        "from nltk.tokenize import PunktSentenceTokenizer\n",
        "\n",
        "train_text = state_union.raw(\"2005-GWBush.txt\")\n",
        "sample_text = state_union.raw(\"2006-GWBush.txt\")\n",
        "\n",
        "# create a custom sentence tokenizer based on\n",
        "# the training text from the 2005 speech\n",
        "custom_sent_tokenizer = PunktSentenceTokenizer(train_text)\n",
        "\n",
        "# tokenizes the sample text using the custom sentence tokenizer\n",
        "tokenized = custom_sent_tokenizer.tokenize(sample_text)"
      ],
      "metadata": {
        "id": "qeUvzy9HBvjs"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def process_countent():\n",
        "  for i in tokenized:\n",
        "    words = nltk.word_tokenize(i)\n",
        "    tagged = nltk.pos_tag(words)\n",
        "\n",
        "    namedEnt = nltk.ne_chunk(tagged, binary=True)\n",
        "\n",
        "    print(namedEnt)"
      ],
      "metadata": {
        "id": "VDV0qYe-Bxe8"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "process_countent()"
      ],
      "metadata": {
        "id": "oWm6KL45CGg8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "\n",
        "for sentence in tokenized:\n",
        "    doc = nlp(sentence)\n",
        "    for ent in doc.ents:\n",
        "        print(ent.text, ent.label_)"
      ],
      "metadata": {
        "id": "RqJ9mIMQHLEO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from spacy import displacy\n",
        "\n",
        "doc = nlp(sample_text)\n",
        "displacy.render(doc, style=\"ent\", jupyter=True)"
      ],
      "metadata": {
        "id": "YCcH220jHvxy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **8. Lemmatizing**"
      ],
      "metadata": {
        "id": "9Kf7Qh9WFeZ1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Lemmatizing reduces a word into its base form."
      ],
      "metadata": {
        "id": "2GgB9fzOKGHa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('wordnet')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xb6lD56MI4wB",
        "outputId": "6c38df52-52c2-4ac8-979f-b3f8831dc4ab"
      },
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 55
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('omw-1.4')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-5KV0vRyI7wB",
        "outputId": "2ea6730c-e2e4-4483-c0bb-5f4629eab074"
      },
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 57
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import WordNetLemmatizer"
      ],
      "metadata": {
        "id": "_GFHLtGpInYp"
      },
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "print(lemmatizer.lemmatize(\"cats\"))\n",
        "print(lemmatizer.lemmatize(\"children\"))            # to singular noun\n",
        "print(lemmatizer.lemmatize(\"better\"))            \n",
        "print(lemmatizer.lemmatize(\"better\", pos='a'))     # indicate pos to adj  \n",
        "print(lemmatizer.lemmatize(\"best\", pos='a'))\n",
        "print(lemmatizer.lemmatize(\"running\"))             # default pos is noun\n",
        "print(lemmatizer.lemmatize(\"running\", pos='v'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-bDks2mnItIJ",
        "outputId": "b1ddb734-b16f-4197-8e52-46a4aae8e870"
      },
      "execution_count": 80,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cat\n",
            "child\n",
            "better\n",
            "good\n",
            "best\n",
            "running\n",
            "run\n"
          ]
        }
      ]
    }
  ]
}
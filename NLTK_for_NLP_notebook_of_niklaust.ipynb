{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "CmVHBGA1rVAW",
        "lv8fhPoR3sCt",
        "pEqRleaZt_XN",
        "WNllMMA26bhj",
        "CMBwD2hoC7JT",
        "esWUcVPDOAMt",
        "AJq07sDDBZUs",
        "9Kf7Qh9WFeZ1",
        "yXnkuTDGPw7C"
      ],
      "authorship_tag": "ABX9TyPY3gQRdjnK9763UburAZpA",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/niklaust/Natural_Language_Processing/blob/main/NLTK_for_NLP_notebook_of_niklaust.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h1><center><b>NLTK with Python 3 for Natural Language Processing</b></center></h1>\n"
      ],
      "metadata": {
        "id": "L3pKWgARrxuW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "youtube: https://www.youtube.com/playlist?list=PLQVvvaa0QuDf2JswnfiGkliBInZnIC4HL\n",
        "\n",
        "teacher: sentdex"
      ],
      "metadata": {
        "id": "oDyCBQhLsCH-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "github:niklaust"
      ],
      "metadata": {
        "id": "FaXC-R8Jr6It"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "start 20230220"
      ],
      "metadata": {
        "id": "xJHeR7PStSkm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **1. Tokenizing words and Sentences**"
      ],
      "metadata": {
        "id": "CmVHBGA1rVAW"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VYG75DG6rJZV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0c8ffaf1-f6c0-4f03-c589-9267d2d81a5f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "NLTK Downloader\n",
            "---------------------------------------------------------------------------\n",
            "    d) Download   l) List    u) Update   c) Config   h) Help   q) Quit\n",
            "---------------------------------------------------------------------------\n",
            "Downloader> q\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ],
      "source": [
        "import nltk\n",
        "\n",
        "nltk.download()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "* **tokenizing** - a form of grouping things: word tokenizers(separates by words), sentence tokenizers(separate by sentence).\n",
        "\n",
        "* **corporas** - a body of text. ex: medical journals, presidential speeches, English language\n",
        "* **lexicon** - like a dictionary. words and their means (STOCK MARKET 'bull' = a person who buys shares hoping to sell them at a higher price later., zoology 'bull' = an uncastrated male bovine animal.)"
      ],
      "metadata": {
        "id": "y0CsYMjFwENF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('punkt')"
      ],
      "metadata": {
        "id": "eV7e-kCWzKaI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import sent_tokenize, word_tokenize\n",
        "\n",
        "example_text = \"\"\"Hello Mr.Smith, how are you doing today? \n",
        "The weather is great and Python is awesome. \n",
        "The sky is pinkish-blue. \n",
        "you should not eat cardboard.\"\"\""
      ],
      "metadata": {
        "id": "RLyV8fQ3vzA2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(sent_tokenize(example_text))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZGvhCmmnzo9N",
        "outputId": "a2f0ec04-6c3e-462c-d6f0-ac14e63361dc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Hello Mr.Smith, how are you doing today?', 'The weather is great and Python is awesome.', 'The sky is pinkish-blue.', 'you should not eat cardboard.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(word_tokenize(example_text))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y1AZnUczzqa2",
        "outputId": "b9308ea6-3cae-46af-cccb-e83ad3c18e0d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Hello', 'Mr.Smith', ',', 'how', 'are', 'you', 'doing', 'today', '?', 'The', 'weather', 'is', 'great', 'and', 'Python', 'is', 'awesome', '.', 'The', 'sky', 'is', 'pinkish-blue', '.', 'you', 'should', 'not', 'eat', 'cardboard', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for i in word_tokenize(example_text):\n",
        "  print(i)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MLepv_gmz46_",
        "outputId": "5889410a-212e-4313-cd5c-224dcdaa0ecd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hello\n",
            "Mr.Smith\n",
            ",\n",
            "how\n",
            "are\n",
            "you\n",
            "doing\n",
            "today\n",
            "?\n",
            "The\n",
            "weather\n",
            "is\n",
            "great\n",
            "and\n",
            "Python\n",
            "is\n",
            "awesome\n",
            ".\n",
            "The\n",
            "sky\n",
            "is\n",
            "pinkish-blue\n",
            ".\n",
            "you\n",
            "should\n",
            "not\n",
            "eat\n",
            "cardboard\n",
            ".\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "valyrio = \"\"\"Nyke Daenerys Jelmazmo \n",
        "hen Targario Lentrot,\n",
        "hen Valyrio Uepo anogar iksan.\n",
        "Valyrio muño engos ñuhys issa.\n",
        "Dovaogedys!\n",
        "Aeksia ossenatas,\n",
        "menti ossenatas,\n",
        "qiloni pilos lue vale tolvie ossenatas,\n",
        "yn riñe dore odrikatas.\n",
        "Urnet luo buzdaro tolvio belma pryjatas!\"\"\"\n",
        "\n",
        "print(sent_tokenize(valyrio.replace('\\n', ' ')))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rQxFfpyx042d",
        "outputId": "347af77d-6278-4d90-b925-65e6c01541e4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Nyke Daenerys Jelmazmo  hen Targario Lentrot, hen Valyrio Uepo anogar iksan.', 'Valyrio muño engos ñuhys issa.', 'Dovaogedys!', 'Aeksia ossenatas, menti ossenatas, qiloni pilos lue vale tolvie ossenatas, yn riñe dore odrikatas.', 'Urnet luo buzdaro tolvio belma pryjatas!']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **2.Stop Words**"
      ],
      "metadata": {
        "id": "lv8fhPoR3sCt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Stop words -  the common **words that are often removed from texts** during pre-processing, as they are considered to carry little to no meaning or have a low semantic value.\n",
        "\n",
        "Examples of stop words include words like \"the\", \"a\", \"an\", \"in\", \"and\", \"is\", \"of\", \"to\", \"that\", and \"it\".  These words often occur frequently in natural language text, but typically do not carry much important meaning or context, and their inclusion can add noise to analysis or machine learning models.\n",
        "\n",
        "By removing stop words from text, the remaining words can be more representative of the meaning and context of the text, which can help improve the accuracy and efficiency of natural language processing tasks such as text classification, sentiment analysis, and information retrieval."
      ],
      "metadata": {
        "id": "ICqN2bku30P9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('stopwords')"
      ],
      "metadata": {
        "id": "4F27tTME5q6d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "example_sentence = \"This is an example showing off stop word filtration.\"\n",
        "\n",
        "stop_words = set(stopwords.words(\"english\"))\n",
        "\n",
        "words = word_tokenize(example_sentence)\n",
        "\n",
        "filtered_sentence = []\n",
        "\n",
        "for w in words:\n",
        "  if w not in stop_words:\n",
        "    filtered_sentence.append(w) \n",
        "\n",
        "print(filtered_sentence)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "emJiVkcy3fVn",
        "outputId": "92792c18-8d00-43c6-c2ff-534c87ed3608"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['This', 'example', 'showing', 'stop', 'word', 'filtration', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "filtered_sentence = [w for w in words if w not in stop_words]\n",
        "\n",
        "print(filtered_sentence)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "biu9ALkc51Qt",
        "outputId": "e504b8dc-22ea-47d8-efa7-46db2cc00e32"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['This', 'example', 'showing', 'stop', 'word', 'filtration', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "meningar = \"\"\"Jag lever livet i nuet.\n",
        "Jag tror att det är bra.\n",
        "Varje minut ät \n",
        "viktig för mig.\n",
        "Det är bra att ha \n",
        "tid för barnen \n",
        "och familjen.\n",
        "Den tiden är \n",
        "mycket viktig \n",
        "för mig.\n",
        "\"\"\"\n",
        "\n",
        "stoppa_ord = list(stopwords.words(\"swedish\")) + list('.')\n",
        "ord = word_tokenize(meningar)\n",
        "print(ord)\n",
        "\n",
        "filtrerad_sats = [o for o in ord if o not in stoppa_ord]\n",
        "\n",
        "print(filtrerad_sats)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nrUecNqL7Aet",
        "outputId": "0fccd509-bc4e-46ce-e048-67371b768a8c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Jag', 'lever', 'livet', 'i', 'nuet', '.', 'Jag', 'tror', 'att', 'det', 'är', 'bra', '.', 'Varje', 'minut', 'ät', 'viktig', 'för', 'mig', '.', 'Det', 'är', 'bra', 'att', 'ha', 'tid', 'för', 'barnen', 'och', 'familjen', '.', 'Den', 'tiden', 'är', 'mycket', 'viktig', 'för', 'mig', '.']\n",
            "['Jag', 'lever', 'livet', 'nuet', 'Jag', 'tror', 'bra', 'Varje', 'minut', 'ät', 'viktig', 'Det', 'bra', 'tid', 'barnen', 'familjen', 'Den', 'tiden', 'viktig']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **3.Stemming**"
      ],
      "metadata": {
        "id": "pEqRleaZt_XN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Stemming** is the process of reducing words to their **base or root form**, in order to normalize them and group them together for analysis. Stemming is commonly used as a preprocessing step in text analysis, to **reduce the number of unique words** that need to be analyzed and improve the accuracy of the analysis.\n",
        "\n",
        "For example, the word \"running\" might be stemmed to \"run\", and the word \"cats\" might be stemmed to \"cat\".\n",
        "\n",
        "\n",
        "* I was taking a ride in the car.  (ride - noun) \n",
        "* I was riding in the car.         (ride - verb)"
      ],
      "metadata": {
        "id": "56msSg2fuuvd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt')"
      ],
      "metadata": {
        "id": "Qv0CWvyewYa8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import PorterStemmer\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "ps = PorterStemmer()\n",
        "\n",
        "example_words = [\"python\", \"pythoner\", \"pythoning\", \"pythoned\", \"pythonly\"]\n",
        "\n",
        "for w in example_words:\n",
        "  print(ps.stem(w))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sPr4yLy4vwgE",
        "outputId": "42e969c2-3e85-490f-967b-490dc18bb15e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "python\n",
            "python\n",
            "python\n",
            "python\n",
            "pythonli\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "new_text = \"\"\"\n",
        "It is very important to be pythonly\n",
        "while you are pythoning with python. \n",
        "All pythoners have pythoned poorly \n",
        "at least once.\n",
        "\"\"\"\n",
        "words = word_tokenize(new_text)\n",
        "\n",
        "for w in words:\n",
        "  print(ps.stem(w))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "srtErj3X0T9F",
        "outputId": "3b563339-4f0d-4e72-b7ea-698206f693c6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "it\n",
            "is\n",
            "veri\n",
            "import\n",
            "to\n",
            "be\n",
            "pythonli\n",
            "while\n",
            "you\n",
            "are\n",
            "python\n",
            "with\n",
            "python\n",
            ".\n",
            "all\n",
            "python\n",
            "have\n",
            "python\n",
            "poorli\n",
            "at\n",
            "least\n",
            "onc\n",
            ".\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem.snowball import SnowballStemmer\n",
        "\n",
        "stemmer = SnowballStemmer('spanish')\n",
        "\n",
        "frase = \"La cantantita le cantó una canción al cantantillo.\"\n",
        "palabras = word_tokenize(frase)\n",
        "\n",
        "# stemming technique \n",
        "palabras_lematizadas = [stemmer.stem(palabra) for palabra in palabras]\n",
        "\n",
        "print(palabras_lematizadas)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IRQQFShl2t_W",
        "outputId": "56d890cd-8d65-4419-f6ef-be04da379621"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['la', 'cantantit', 'le', 'cant', 'una', 'cancion', 'al', 'cantantill', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "import spacy.cli\n",
        "\n",
        "spacy.cli.download(\"es_core_news_sm\")"
      ],
      "metadata": {
        "id": "tM2IFndS79rj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nlp = spacy.load(\"es_core_news_sm\")\n",
        "\n",
        "doc = nlp(\"La cantantita le cantó una canción al cantantillo\")\n",
        "\n",
        "# lemmatization technique\n",
        "lemmatized_words = [token.lemma_ for token in doc]\n",
        "\n",
        "print(lemmatized_words)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eEflBmWB7Ue0",
        "outputId": "f0412b85-f32d-4c63-eb0b-6c7107523acf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['el', 'cantantita', 'él', 'cantar', 'uno', 'canción', 'al', 'cantantillo']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **4. Part of Speech Tagging**"
      ],
      "metadata": {
        "id": "WNllMMA26bhj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<table>\n",
        "  <tr>\n",
        "    <th colspan=\"2\"><h4>Part-of-Speech(POS) tag</h4></th>\n",
        "  </tr>\n",
        "    <th></th>\n",
        "    <th></th>\n",
        "  </tr>\n",
        "  <tr>\n",
        "    <td>CC:</td>\n",
        "    <td>coordinating conjunction</td>\n",
        "  </tr>\n",
        "  <tr>\n",
        "    <td>CD:</td>\n",
        "    <td>cardinal digit</td>\n",
        "  </tr>\n",
        "  <tr>\n",
        "    <td>DT:</td>\n",
        "    <td>determiner</td>\n",
        "  </tr>\n",
        "  <tr>\n",
        "    <td>EX:</td>\n",
        "    <td>existential there (like: \"there is\" ... think of it like \"there exists\")</td>\n",
        "  </tr>\n",
        "  <tr>\n",
        "    <td>FW:</td>\n",
        "    <td>foreign word</td>\n",
        "  </tr>\n",
        "  <tr>\n",
        "    <td>IN:</td>\n",
        "    <td>preposition/subordinating conjunction</td>\n",
        "  </tr>\n",
        "  <tr>\n",
        "    <td>JJ:</td>\n",
        "    <td>adjective 'big'</td>\n",
        "  </tr>\n",
        "  <tr>\n",
        "    <td>JJR:</td>\n",
        "    <td>adjective, comparative 'bigger'</td>\n",
        "  </tr>\n",
        "  <tr>\n",
        "    <td>JJS:</td>\n",
        "    <td>adjective, superlative 'biggest'</td>\n",
        "  </tr>\n",
        "  <tr>\n",
        "    <td>LS:</td>\n",
        "    <td>list marker 1)</td>\n",
        "  </tr>\n",
        "  <tr>\n",
        "    <td>MD:</td>\n",
        "    <td>modal could, will</td>\n",
        "  </tr>\n",
        "  <tr>\n",
        "    <td>NN:</td>\n",
        "    <td>noun, singular 'desk'</td>\n",
        "  </tr>\n",
        "  <tr>\n",
        "    <td>NNS:</td>\n",
        "    <td>noun plural 'desks'</td>\n",
        "  </tr>\n",
        "  <tr>\n",
        "    <td>NNP:</td>\n",
        "    <td>proper noun, singular 'Harrison'</td>\n",
        "  </tr>\n",
        "  <tr>\n",
        "    <td>NNPS:</td>\n",
        "    <td>proper noun, plural 'Americans'</td>\n",
        "  </tr>\n",
        "  <tr>\n",
        "    <td>PDT:</td>\n",
        "    <td>predeterminer 'all the kids'</td>\n",
        "  </tr>\n",
        "  <tr>\n",
        "    <td>POS:</td>\n",
        "    <td>possessive ending parent's</td>\n",
        "  </tr>\n",
        "  <tr>\n",
        "    <td>PRP:</td>\n",
        "    <td>personal pronoun I, he, she</td>\n",
        "  </tr>\n",
        "  <tr>\n",
        "    <td>PRP$:</td>\n",
        "    <td>possessive pronoun my, his, hers</td>\n",
        "  </tr>\n",
        "  <tr>\n",
        "    <td>RB:</td>\n",
        "    <td>adverb very, silently,</td>\n",
        "  </tr>\n",
        "  <tr>\n",
        "    <td>RBR:</td>\n",
        "    <td>adverb, comparative better</td>\n",
        "  </tr>\n",
        "  <tr>\n",
        "    <td>RBS:</td>\n",
        "    <td>adverb, superlative best</td>\n",
        "  </tr>\n",
        "  <tr>\n",
        "    <td>RP:</td>\n",
        "    <td>particle give up</td>\n",
        "  </tr>\n",
        "  <tr>\n",
        "    <td>TO:</td>\n",
        "    <td>to go 'to' the store.</td>\n",
        "  </tr>\n",
        "  <tr>\n",
        "    <td>UH:</td>\n",
        "    <td>interjection uhhuhhuhh</td>\n",
        "  </tr>\n",
        "  <tr>\n",
        "    <td>VB:</td>\n",
        "    <td>verb, base form take</td>\n",
        "  </tr>\n",
        "  <tr>\n",
        "    <td>VBD:</td>\n",
        "    <td>verb, past tense took</td>\n",
        "  </tr>\n",
        "  <tr>\n",
        "    <td>VBG:</td>\n",
        "    <td>verb, gerund/present participle taking</td>\n",
        "  </tr>\n",
        "  <tr>\n",
        "    <td>VBN:</td>\n",
        "    <td>verb, past participle taken</td>\n",
        "  </tr>\n",
        "  <tr>\n",
        "    <td>VBP:</td>\n",
        "    <td>verb, sing. present, non-3d take</td>\n",
        "  </tr>\n",
        "  <tr>\n",
        "    <td>VBZ:</td>\n",
        "    <td>verb, 3rd person sing. present takes</td>\n",
        "  </tr>\n",
        "  <tr>\n",
        "    <td>WDT:</td>\n",
        "    <td>wh-determiner which</td>\n",
        "  </tr>\n",
        "  <tr>\n",
        "    <td>WP:</td>\n",
        "    <td>wh-pronoun who, what</td>\n",
        "  </tr>\n",
        "  <tr>\n",
        "    <td>WP$:</td>\n",
        "    <td>possessive wh-pronoun whose</td>\n",
        "  </tr>\n",
        "  <tr>\n",
        "    <td>WRB:</td>\n",
        "    <td>wh-abverb where, when</td>\n",
        "  </tr>\n",
        "</table>"
      ],
      "metadata": {
        "id": "wvDpu1AaBIik"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('state_union')\n",
        "nltk.download('averaged_perceptron_tagger')   # pos_tag"
      ],
      "metadata": {
        "id": "HFew_XFi-4ND"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "`PunktSentenceTokenizer` is a sentence tokenizer in NLTK.\n",
        "It is based on unsupervised machine learning algorithm Punkt. It can be trained on a large corpus of text to determine sentence boundaries. Once trained, it can be used to tokenize new text into sentences for further analysis."
      ],
      "metadata": {
        "id": "Nk08jHX0OCt7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import state_union\n",
        "from nltk.tokenize import PunktSentenceTokenizer\n",
        "\n",
        "train_text = state_union.raw(\"2005-GWBush.txt\")\n",
        "sample_text = state_union.raw(\"2006-GWBush.txt\")\n",
        "\n",
        "# create a custom sentence tokenizer based on\n",
        "# the training text from the 2005 speech\n",
        "custom_sent_tokenizer = PunktSentenceTokenizer(train_text)\n",
        "\n",
        "# tokenizes the sample text using the custom sentence tokenizer\n",
        "tokenized = custom_sent_tokenizer.tokenize(sample_text)\n",
        "\n",
        "def process_countent():\n",
        "  try:\n",
        "      for i in tokenized:\n",
        "        words = nltk.word_tokenize(i)\n",
        "        tagged = nltk.pos_tag(words)   # pos tagging on text data.\n",
        "        print(tagged)\n",
        "\n",
        "  except Exception as e:\n",
        "    print(str(e))"
      ],
      "metadata": {
        "id": "SNqeyxlA9V4M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenized[:5]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7UECL5vtFUjr",
        "outputId": "388edf92-4615-4484-c4e8-697c4f9e27a1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[\"PRESIDENT GEORGE W. BUSH'S ADDRESS BEFORE A JOINT SESSION OF THE CONGRESS ON THE STATE OF THE UNION\\n \\nJanuary 31, 2006\\n\\nTHE PRESIDENT: Thank you all.\",\n",
              " 'Mr. Speaker, Vice President Cheney, members of Congress, members of the Supreme Court and diplomatic corps, distinguished guests, and fellow citizens: Today our nation lost a beloved, graceful, courageous woman who called America to its founding ideals and carried on a noble dream.',\n",
              " 'Tonight we are comforted by the hope of a glad reunion with the husband who was taken so long ago, and we are grateful for the good life of Coretta Scott King.',\n",
              " '(Applause.)',\n",
              " 'President George W. Bush reacts to applause during his State of the Union Address at the Capitol, Tuesday, Jan.']"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "process_countent()"
      ],
      "metadata": {
        "id": "ZTlxUFlKFADE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **5. Chunking**"
      ],
      "metadata": {
        "id": "CMBwD2hoC7JT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Chunking is the process of **extracting** meaningful phrases (or \"chunks\") **from a text by identifying and labeling sequences of words** that have a specific grammatical structure."
      ],
      "metadata": {
        "id": "y_74LmRVQDLE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk"
      ],
      "metadata": {
        "id": "Z0XUVaGeNSz1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('punkt')"
      ],
      "metadata": {
        "id": "yBZhVX8bFfF4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('state_union')\n",
        "nltk.download('averaged_perceptron_tagger')   # pos_tag"
      ],
      "metadata": {
        "id": "jOuY204NNYtm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.tokenize import word_tokenize"
      ],
      "metadata": {
        "id": "veXgba47IbVu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.corpus import state_union\n",
        "from nltk.tokenize import PunktSentenceTokenizer\n",
        "\n",
        "train_text = state_union.raw(\"2005-GWBush.txt\")\n",
        "sample_text = state_union.raw(\"2006-GWBush.txt\")\n",
        "\n",
        "# create a custom sentence tokenizer based on\n",
        "# the training text from the 2005 speech\n",
        "custom_sent_tokenizer = PunktSentenceTokenizer(train_text)\n",
        "\n",
        "# tokenizes the sample text using the custom sentence tokenizer\n",
        "tokenized = custom_sent_tokenizer.tokenize(sample_text)"
      ],
      "metadata": {
        "id": "I2dOdas_Neen"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def process_countent():\n",
        "  for i in tokenized:\n",
        "    words = nltk.word_tokenize(i)\n",
        "    tagged = nltk.pos_tag(words)\n",
        "\n",
        "    chunkGram = r\"\"\"Chunk: {<RB.?>*<VB.?>*<NNP>+<NN>?}\"\"\"\n",
        "\n",
        "    chunkParser = nltk.RegexpParser(chunkGram)\n",
        "    chunked = chunkParser.parse(tagged)\n",
        "\n",
        "    print(chunked)"
      ],
      "metadata": {
        "id": "Q5tdgyBcD94I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "process_countent()"
      ],
      "metadata": {
        "id": "vrkAvRJtFQ9y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **6. Chinking**"
      ],
      "metadata": {
        "id": "esWUcVPDOAMt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Chinking is the process of **removing** a sequence of tokens (or \"chunks\") **from a larger text that match a specific pattern.** The pattern is defined using a regular expression, just like in chunking."
      ],
      "metadata": {
        "id": "a-g-OtXbQfnM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk"
      ],
      "metadata": {
        "id": "qlXrr6zpOjcW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('punkt')"
      ],
      "metadata": {
        "id": "LMo00UcfOlF9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('state_union')\n",
        "nltk.download('averaged_perceptron_tagger')   # pos_tag"
      ],
      "metadata": {
        "id": "chMhFe8AOmoF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.tokenize import word_tokenize"
      ],
      "metadata": {
        "id": "RLJgfQ3JOoQ_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.corpus import state_union\n",
        "from nltk.tokenize import PunktSentenceTokenizer\n",
        "\n",
        "train_text = state_union.raw(\"2005-GWBush.txt\")\n",
        "sample_text = state_union.raw(\"2006-GWBush.txt\")\n",
        "\n",
        "# create a custom sentence tokenizer based on\n",
        "# the training text from the 2005 speech\n",
        "custom_sent_tokenizer = PunktSentenceTokenizer(train_text)\n",
        "\n",
        "# tokenizes the sample text using the custom sentence tokenizer\n",
        "tokenized = custom_sent_tokenizer.tokenize(sample_text)"
      ],
      "metadata": {
        "id": "I-UwTCUUOp6U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def process_countent():\n",
        "  for i in tokenized:\n",
        "    words = nltk.word_tokenize(i)\n",
        "    tagged = nltk.pos_tag(words)\n",
        "\n",
        "    chunkGram = r\"\"\"Chunk: {<.*>+}\n",
        "                            }<VB.?|IN|DT|TO>+{\"\"\"\n",
        "\n",
        "    chunkParser = nltk.RegexpParser(chunkGram)\n",
        "    chunked = chunkParser.parse(tagged)\n",
        "\n",
        "    print(chunked)"
      ],
      "metadata": {
        "id": "JhvSkAHAOrcV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "process_countent()"
      ],
      "metadata": {
        "id": "t4mOAdK2O9mo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **7. Named Entity Recognition**"
      ],
      "metadata": {
        "id": "AJq07sDDBZUs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<table>\n",
        "  <tr>\n",
        "    <th colspan=\"9\"><h4>Commonly used types of named entity</h4></th>\n",
        "  </tr>\n",
        "  <tr>\n",
        "    <th>NE type</th>\n",
        "    <th>Examples</th>\n",
        "  </tr>\n",
        "  <tr>\n",
        "    <td>ORGANIZATION</td>\n",
        "    <td>Georgia-Pacific Corp., WHO</td>\n",
        "  </tr>\n",
        "  <tr>\n",
        "    <td>PERSON</td>\n",
        "    <td>Eddy Bonte, Persident Obama</td>\n",
        "  </tr>\n",
        "  <tr>\n",
        "    <td>LOCATION</td>\n",
        "    <td>Murray River, Mount Everest</td>\n",
        "  </tr>\n",
        "  <tr>\n",
        "    <td>DATE</td>\n",
        "    <td>June, 2008-06-29</td>\n",
        "  </tr>\n",
        "  <tr>\n",
        "    <td>TIME</td>\n",
        "    <td>two fifty a m, 1:30 p.m.</td>\n",
        "  </tr>\n",
        "  <tr>\n",
        "    <td>MONEY</td>\n",
        "    <td>175 million Canadian Dollars, GBP 10.40</td>\n",
        "  </tr>\n",
        "  <tr>\n",
        "    <td>PERCENT</td>\n",
        "    <td>twenty pct, 18.75%</td>\n",
        "  </tr>\n",
        "  <tr>\n",
        "    <td>FACILITY</td>\n",
        "    <td>Washington Monument, Stonehenge</td>\n",
        "  </tr>\n",
        "  <tr>\n",
        "    <td>GPE</td>\n",
        "    <td>South East Asia, Midlothian</td>\n",
        "  </tr>\n",
        "</table>"
      ],
      "metadata": {
        "id": "nEk6uWJkEI50"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk"
      ],
      "metadata": {
        "id": "4BdEmHWmBoz8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('punkt')"
      ],
      "metadata": {
        "id": "1oQJiId6BqZD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('state_union')\n",
        "nltk.download('averaged_perceptron_tagger')   # pos_tag"
      ],
      "metadata": {
        "id": "eEDWlRcvBr11"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('words')"
      ],
      "metadata": {
        "id": "I_IoqkHRCXhk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('maxent_ne_chunker')"
      ],
      "metadata": {
        "id": "eJGWOdZGCRY0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.tokenize import word_tokenize"
      ],
      "metadata": {
        "id": "IeN-3i7MBtc9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.corpus import state_union\n",
        "from nltk.tokenize import PunktSentenceTokenizer\n",
        "\n",
        "train_text = state_union.raw(\"2005-GWBush.txt\")\n",
        "sample_text = state_union.raw(\"2006-GWBush.txt\")\n",
        "\n",
        "# create a custom sentence tokenizer based on\n",
        "# the training text from the 2005 speech\n",
        "custom_sent_tokenizer = PunktSentenceTokenizer(train_text)\n",
        "\n",
        "# tokenizes the sample text using the custom sentence tokenizer\n",
        "tokenized = custom_sent_tokenizer.tokenize(sample_text)"
      ],
      "metadata": {
        "id": "qeUvzy9HBvjs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def process_countent():\n",
        "  for i in tokenized:\n",
        "    words = nltk.word_tokenize(i)\n",
        "    tagged = nltk.pos_tag(words)\n",
        "\n",
        "    namedEnt = nltk.ne_chunk(tagged, binary=True)\n",
        "\n",
        "    print(namedEnt)"
      ],
      "metadata": {
        "id": "VDV0qYe-Bxe8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "process_countent()"
      ],
      "metadata": {
        "id": "oWm6KL45CGg8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "\n",
        "for sentence in tokenized:\n",
        "    doc = nlp(sentence)\n",
        "    for ent in doc.ents:\n",
        "        print(ent.text, ent.label_)"
      ],
      "metadata": {
        "id": "RqJ9mIMQHLEO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from spacy import displacy\n",
        "\n",
        "doc = nlp(sample_text)\n",
        "displacy.render(doc, style=\"ent\", jupyter=True)"
      ],
      "metadata": {
        "id": "YCcH220jHvxy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **8. Lemmatizing**"
      ],
      "metadata": {
        "id": "9Kf7Qh9WFeZ1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Lemmatizing reduces a word into its base form."
      ],
      "metadata": {
        "id": "2GgB9fzOKGHa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('wordnet')"
      ],
      "metadata": {
        "id": "xb6lD56MI4wB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('omw-1.4')"
      ],
      "metadata": {
        "id": "-5KV0vRyI7wB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import WordNetLemmatizer"
      ],
      "metadata": {
        "id": "_GFHLtGpInYp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "print(lemmatizer.lemmatize(\"cats\"))\n",
        "print(lemmatizer.lemmatize(\"children\"))            # to singular noun\n",
        "print(lemmatizer.lemmatize(\"better\"))            \n",
        "print(lemmatizer.lemmatize(\"better\", pos='a'))     # indicate pos to adj  \n",
        "print(lemmatizer.lemmatize(\"best\", pos='a'))\n",
        "print(lemmatizer.lemmatize(\"running\"))             # default pos is noun\n",
        "print(lemmatizer.lemmatize(\"running\", pos='v'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-bDks2mnItIJ",
        "outputId": "b1ddb734-b16f-4197-8e52-46a4aae8e870"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cat\n",
            "child\n",
            "better\n",
            "good\n",
            "best\n",
            "running\n",
            "run\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **9. NLTK Corpora**"
      ],
      "metadata": {
        "id": "yXnkuTDGPw7C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk"
      ],
      "metadata": {
        "id": "WL8Gyxa_Q8UN"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# see the location of nltk file stored in the machine\n",
        "print(nltk.__file__)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8QFcNlEFQ-QU",
        "outputId": "c3c545a6-575a-4562-d20e-79c216cf3d69"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/nltk/__init__.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('gutenberg')\n",
        "nltk.download('punkt')"
      ],
      "metadata": {
        "id": "WqZW4UU4RmXJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.corpus import gutenberg\n",
        "from nltk.tokenize import sent_tokenize"
      ],
      "metadata": {
        "id": "Sttr7kr5SfJQ"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.corpus.gutenberg.fileids()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZwGDEClBT46n",
        "outputId": "a82decf8-af4c-49f4-ed76-eafaf11e9adb"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['austen-emma.txt',\n",
              " 'austen-persuasion.txt',\n",
              " 'austen-sense.txt',\n",
              " 'bible-kjv.txt',\n",
              " 'blake-poems.txt',\n",
              " 'bryant-stories.txt',\n",
              " 'burgess-busterbrown.txt',\n",
              " 'carroll-alice.txt',\n",
              " 'chesterton-ball.txt',\n",
              " 'chesterton-brown.txt',\n",
              " 'chesterton-thursday.txt',\n",
              " 'edgeworth-parents.txt',\n",
              " 'melville-moby_dick.txt',\n",
              " 'milton-paradise.txt',\n",
              " 'shakespeare-caesar.txt',\n",
              " 'shakespeare-hamlet.txt',\n",
              " 'shakespeare-macbeth.txt',\n",
              " 'whitman-leaves.txt']"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sample = gutenberg.raw(\"bible-kjv.txt\")\n",
        "token = sent_tokenize(sample)"
      ],
      "metadata": {
        "id": "xD0FsxPISkow"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pprint import pprint\n",
        "pprint(token[5:15])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YkcS7R5vTTqi",
        "outputId": "12889299-f469-4b6d-ad4e-a1d06100c447"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['1:5 And God called the light Day, and the darkness he called Night.',\n",
            " 'And the evening and the morning were the first day.',\n",
            " '1:6 And God said, Let there be a firmament in the midst of the waters,\\n'\n",
            " 'and let it divide the waters from the waters.',\n",
            " '1:7 And God made the firmament, and divided the waters which were\\n'\n",
            " 'under the firmament from the waters which were above the firmament:\\n'\n",
            " 'and it was so.',\n",
            " '1:8 And God called the firmament Heaven.',\n",
            " 'And the evening and the\\nmorning were the second day.',\n",
            " '1:9 And God said, Let the waters under the heaven be gathered together\\n'\n",
            " 'unto one place, and let the dry land appear: and it was so.',\n",
            " '1:10 And God called the dry land Earth; and the gathering together of\\n'\n",
            " 'the waters called he Seas: and God saw that it was good.',\n",
            " '1:11 And God said, Let the earth bring forth grass, the herb yielding\\n'\n",
            " 'seed, and the fruit tree yielding fruit after his kind, whose seed is\\n'\n",
            " 'in itself, upon the earth: and it was so.',\n",
            " '1:12 And the earth brought forth grass, and herb yielding seed after\\n'\n",
            " 'his kind, and the tree yielding fruit, whose seed was in itself, after\\n'\n",
            " 'his kind: and God saw that it was good.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **10. WordNet**"
      ],
      "metadata": {
        "id": "SEDZi3DMUX2I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('wordnet')"
      ],
      "metadata": {
        "id": "4EbaTOkWUqzI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('omw-1.4')"
      ],
      "metadata": {
        "id": "La8SwFMzU6up"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.corpus import wordnet"
      ],
      "metadata": {
        "id": "jeVfpH8WUpW_"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "syns = wordnet.synsets(\"program\")"
      ],
      "metadata": {
        "id": "ae0ZuQObUzxH"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pprint(syns)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s6W9BVbEVC5v",
        "outputId": "a9b4afc5-d86d-49f7-8761-28087834c98a"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Synset('plan.n.01'),\n",
            " Synset('program.n.02'),\n",
            " Synset('broadcast.n.02'),\n",
            " Synset('platform.n.02'),\n",
            " Synset('program.n.05'),\n",
            " Synset('course_of_study.n.01'),\n",
            " Synset('program.n.07'),\n",
            " Synset('program.n.08'),\n",
            " Synset('program.v.01'),\n",
            " Synset('program.v.02')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# synset\n",
        "print(syns[0].name())\n",
        "\n",
        "# just the word\n",
        "print(syns[0].lemmas()[0].name())\n",
        "\n",
        "# definition\n",
        "print(syns[0].definition())\n",
        "\n",
        "# example\n",
        "print(syns[0].examples())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R5tWaQBUVO7-",
        "outputId": "bc4adc56-8392-424d-a82d-38068a92ac5a"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "plan.n.01\n",
            "plan\n",
            "a series of steps to be carried out or goals to be accomplished\n",
            "['they drew up a six-step plan', 'they discussed plans for a new bond issue']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "synonyms = []\n",
        "antonyms = []\n",
        "\n",
        "for syn in wordnet.synsets('good'):\n",
        "  for l in syn.lemmas():\n",
        "    synonyms.append(l.name())\n",
        "    if l.antonyms():\n",
        "      antonyms.append(l.antonyms()[0].name())\n",
        "\n",
        "print(f\"sysnonyms:\\n{set(synonyms)}\")\n",
        "print(f\"antonyms:\\n{set(antonyms)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y3yWqD7iWBl3",
        "outputId": "408f89b6-5df7-499d-f68b-701b3a3e6eb8"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "sysnonyms:\n",
            "{'trade_good', 'just', 'commodity', 'skillful', 'secure', 'serious', 'safe', 'expert', 'sound', 'dear', 'thoroughly', 'ripe', 'good', 'in_force', 'in_effect', 'unspoilt', 'undecomposed', 'well', 'honorable', 'upright', 'goodness', 'adept', 'honest', 'proficient', 'beneficial', 'effective', 'practiced', 'estimable', 'right', 'dependable', 'near', 'full', 'soundly', 'salutary', 'respectable', 'skilful', 'unspoiled'}\n",
            "antonyms:\n",
            "{'evilness', 'badness', 'bad', 'ill', 'evil'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ship = wordnet.synset(\"ship.n.01\")"
      ],
      "metadata": {
        "id": "lsdaFFcLZtxm"
      },
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# hyponyms give the concepts that are more specific\n",
        "ship.hyponyms()"
      ],
      "metadata": {
        "id": "b2o-m_k4abaQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# hyponyms give the concepts that are more broad\n",
        "ship.hypernyms()"
      ],
      "metadata": {
        "id": "Uv55hwRlav3t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# meronyms items to their components\n",
        "ship.part_meronyms()"
      ],
      "metadata": {
        "id": "HFWAM5sJa6st"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# holonyms the things they are contained in\n",
        "ship.member_holonyms()"
      ],
      "metadata": {
        "id": "0PNnFF96bGoG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# entailments show close relationships between verbs\n",
        "walk = wordnet.synset('walk.v.01')\n",
        "walk.entailments()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VrwwXhmSeK1T",
        "outputId": "a4899dce-c618-461a-9597-0f49f056229d"
      },
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Synset('step.v.01')]"
            ]
          },
          "metadata": {},
          "execution_count": 74
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**word similarity**"
      ],
      "metadata": {
        "id": "jUblg-TBaRLe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Wu-Palmer Similarity: Return a score denoting how similar two word senses are,\n",
        "# based on the depth of the two senses in the taxonomy and that of their Least \n",
        "# Common Subsumer (most specific ancestor node)\n",
        "w1 = wordnet.synset(\"ship.n.01\")\n",
        "w2 = wordnet.synset(\"boat.n.01\")\n",
        "print(w1.wup_similarity(w2))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V71p3IcDX094",
        "outputId": "3c5accb3-5261-4e1f-b019-b908837a9621"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.9090909090909091\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# shortest path that connects the concepts in the hypernym hierarchy\n",
        "print(w1.path_similarity(w2))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1myZd7wQYbE9",
        "outputId": "f457ac72-0a4e-4200-9f40-c71e4df5e4a0"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.3333333333333333\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "w1 = wordnet.synset(\"ship.n.01\")\n",
        "w2 = wordnet.synset(\"car.n.01\")\n",
        "print(w1.wup_similarity(w2))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Za1KqSy_ZCPN",
        "outputId": "dd889991-f31c-471e-8112-19f414b02a2e"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.6956521739130435\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(w1.path_similarity(w2))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YAV7lBl9ZLG9",
        "outputId": "65fdbfe5-c22a-43b3-a1f3-a78b896b5d3b"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.125\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "w1 = wordnet.synset(\"ship.n.01\")\n",
        "w2 = wordnet.synset(\"cat.n.01\")\n",
        "print(w1.wup_similarity(w2))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rL-bPN4DZD6F",
        "outputId": "3c484964-fad7-477e-b49e-af35ffd17f94"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.32\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(w1.path_similarity(w2))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "52m0lObUZMhR",
        "outputId": "d6dbcee7-3b77-4265-fcb6-c86641901a3b"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.05555555555555555\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "w1 = wordnet.synset(\"ship.n.01\")\n",
        "w2 = wordnet.synset(\"cactus.n.01\")\n",
        "print(w1.wup_similarity(w2))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "At3yzxQfZYem",
        "outputId": "5f682330-ca88-4d4f-a90d-bc5a90dca16b"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.38095238095238093\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(w1.path_similarity(w2))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9Au2arkrZasE",
        "outputId": "65047d8d-8554-481a-8b71-5fa20b12b5b7"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.07142857142857142\n"
          ]
        }
      ]
    }
  ]
}
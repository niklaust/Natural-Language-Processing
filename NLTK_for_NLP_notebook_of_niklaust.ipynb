{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPfY/rxhpSmZLl5VUfzgp7K",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/niklaust/Natural_Language_Processing/blob/main/NLTK_for_NLP_notebook_of_niklaust.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h1><center><b>NLTK with Python 3 for Natural Language Processing</b></center></h1>\n"
      ],
      "metadata": {
        "id": "L3pKWgARrxuW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "youtube: https://www.youtube.com/playlist?list=PLQVvvaa0QuDf2JswnfiGkliBInZnIC4HL\n",
        "\n",
        "teacher: sentdex"
      ],
      "metadata": {
        "id": "oDyCBQhLsCH-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "github:niklaust"
      ],
      "metadata": {
        "id": "FaXC-R8Jr6It"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "start 20230220"
      ],
      "metadata": {
        "id": "xJHeR7PStSkm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **1. Tokenizing words and Sentences**"
      ],
      "metadata": {
        "id": "CmVHBGA1rVAW"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "VYG75DG6rJZV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0c8ffaf1-f6c0-4f03-c589-9267d2d81a5f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "NLTK Downloader\n",
            "---------------------------------------------------------------------------\n",
            "    d) Download   l) List    u) Update   c) Config   h) Help   q) Quit\n",
            "---------------------------------------------------------------------------\n",
            "Downloader> q\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ],
      "source": [
        "import nltk\n",
        "\n",
        "nltk.download()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "* **tokenizing** - a form of grouping things: word tokenizers(separates by words), sentence tokenizers(separate by sentence).\n",
        "\n",
        "* **corporas** - a body of text. ex: medical journals, presidential speeches, English language\n",
        "* **lexicon** - like a dictionary. words and their means (STOCK MARKET 'bull' = a person who buys shares hoping to sell them at a higher price later., zoology 'bull' = an uncastrated male bovine animal.)"
      ],
      "metadata": {
        "id": "y0CsYMjFwENF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('punkt')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eV7e-kCWzKaI",
        "outputId": "e1900454-5e58-4a4a-d7a9-2d8c4cfa05fc"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import sent_tokenize, word_tokenize\n",
        "\n",
        "example_text = \"\"\"Hello Mr.Smith, how are you doing today? \n",
        "The weather is great and Python is awesome. \n",
        "The sky is pinkish-blue. \n",
        "you should not eat cardboard.\"\"\""
      ],
      "metadata": {
        "id": "RLyV8fQ3vzA2"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(sent_tokenize(example_text))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZGvhCmmnzo9N",
        "outputId": "a2f0ec04-6c3e-462c-d6f0-ac14e63361dc"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Hello Mr.Smith, how are you doing today?', 'The weather is great and Python is awesome.', 'The sky is pinkish-blue.', 'you should not eat cardboard.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(word_tokenize(example_text))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y1AZnUczzqa2",
        "outputId": "b9308ea6-3cae-46af-cccb-e83ad3c18e0d"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Hello', 'Mr.Smith', ',', 'how', 'are', 'you', 'doing', 'today', '?', 'The', 'weather', 'is', 'great', 'and', 'Python', 'is', 'awesome', '.', 'The', 'sky', 'is', 'pinkish-blue', '.', 'you', 'should', 'not', 'eat', 'cardboard', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for i in word_tokenize(example_text):\n",
        "  print(i)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MLepv_gmz46_",
        "outputId": "5889410a-212e-4313-cd5c-224dcdaa0ecd"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hello\n",
            "Mr.Smith\n",
            ",\n",
            "how\n",
            "are\n",
            "you\n",
            "doing\n",
            "today\n",
            "?\n",
            "The\n",
            "weather\n",
            "is\n",
            "great\n",
            "and\n",
            "Python\n",
            "is\n",
            "awesome\n",
            ".\n",
            "The\n",
            "sky\n",
            "is\n",
            "pinkish-blue\n",
            ".\n",
            "you\n",
            "should\n",
            "not\n",
            "eat\n",
            "cardboard\n",
            ".\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "valyrio = \"\"\"Nyke Daenerys Jelmazmo \n",
        "hen Targario Lentrot,\n",
        "hen Valyrio Uepo anogar iksan.\n",
        "Valyrio muño engos ñuhys issa.\n",
        "Dovaogedys!\n",
        "Aeksia ossenatas,\n",
        "menti ossenatas,\n",
        "qiloni pilos lue vale tolvie ossenatas,\n",
        "yn riñe dore odrikatas.\n",
        "Urnet luo buzdaro tolvio belma pryjatas!\"\"\"\n",
        "\n",
        "print(sent_tokenize(valyrio.replace('\\n', ' ')))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rQxFfpyx042d",
        "outputId": "347af77d-6278-4d90-b925-65e6c01541e4"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Nyke Daenerys Jelmazmo  hen Targario Lentrot, hen Valyrio Uepo anogar iksan.', 'Valyrio muño engos ñuhys issa.', 'Dovaogedys!', 'Aeksia ossenatas, menti ossenatas, qiloni pilos lue vale tolvie ossenatas, yn riñe dore odrikatas.', 'Urnet luo buzdaro tolvio belma pryjatas!']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **2.Stop Words**"
      ],
      "metadata": {
        "id": "lv8fhPoR3sCt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Stop words -  the common **words that are often removed from texts** during pre-processing, as they are considered to carry little to no meaning or have a low semantic value.\n",
        "\n",
        "Examples of stop words include words like \"the\", \"a\", \"an\", \"in\", \"and\", \"is\", \"of\", \"to\", \"that\", and \"it\".  These words often occur frequently in natural language text, but typically do not carry much important meaning or context, and their inclusion can add noise to analysis or machine learning models.\n",
        "\n",
        "By removing stop words from text, the remaining words can be more representative of the meaning and context of the text, which can help improve the accuracy and efficiency of natural language processing tasks such as text classification, sentiment analysis, and information retrieval."
      ],
      "metadata": {
        "id": "ICqN2bku30P9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('stopwords')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4F27tTME5q6d",
        "outputId": "82765797-cc82-4e19-9272-cb6ac416a078"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "example_sentence = \"This is an example showing off stop word filtration.\"\n",
        "\n",
        "stop_words = set(stopwords.words(\"english\"))\n",
        "\n",
        "words = word_tokenize(example_sentence)\n",
        "\n",
        "filtered_sentence = []\n",
        "\n",
        "for w in words:\n",
        "  if w not in stop_words:\n",
        "    filtered_sentence.append(w) \n",
        "\n",
        "print(filtered_sentence)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "emJiVkcy3fVn",
        "outputId": "92792c18-8d00-43c6-c2ff-534c87ed3608"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['This', 'example', 'showing', 'stop', 'word', 'filtration', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "filtered_sentence = [w for w in words if w not in stop_words]\n",
        "\n",
        "print(filtered_sentence)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "biu9ALkc51Qt",
        "outputId": "e504b8dc-22ea-47d8-efa7-46db2cc00e32"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['This', 'example', 'showing', 'stop', 'word', 'filtration', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "meningar = \"\"\"Jag lever livet i nuet.\n",
        "Jag tror att det är bra.\n",
        "Varje minut ät \n",
        "viktig för mig.\n",
        "Det är bra att ha \n",
        "tid för barnen \n",
        "och familjen.\n",
        "Den tiden är \n",
        "mycket viktig \n",
        "för mig.\n",
        "\"\"\"\n",
        "\n",
        "stoppa_ord = list(stopwords.words(\"swedish\")) + list('.')\n",
        "ord = word_tokenize(meningar)\n",
        "print(ord)\n",
        "\n",
        "filtrerad_sats = [o for o in ord if o not in stoppa_ord]\n",
        "\n",
        "print(filtrerad_sats)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nrUecNqL7Aet",
        "outputId": "0fccd509-bc4e-46ce-e048-67371b768a8c"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Jag', 'lever', 'livet', 'i', 'nuet', '.', 'Jag', 'tror', 'att', 'det', 'är', 'bra', '.', 'Varje', 'minut', 'ät', 'viktig', 'för', 'mig', '.', 'Det', 'är', 'bra', 'att', 'ha', 'tid', 'för', 'barnen', 'och', 'familjen', '.', 'Den', 'tiden', 'är', 'mycket', 'viktig', 'för', 'mig', '.']\n",
            "['Jag', 'lever', 'livet', 'nuet', 'Jag', 'tror', 'bra', 'Varje', 'minut', 'ät', 'viktig', 'Det', 'bra', 'tid', 'barnen', 'familjen', 'Den', 'tiden', 'viktig']\n"
          ]
        }
      ]
    }
  ]
}